# Stark-Jr.v0.5

A childhood idea turned into a working local AI system.  Stark-Jr is a real-time, fully offline voice assistant powered by Faster-Whisper, Piper TTS, and local LLMs running through Ollama. It listens, thinks, and talks â€” all without touching the internet. 
external servers.

# Features

Completely offline â€” STT, LLM inference, and TTS run locally.

Fast, low-latency STT using Faster-Whisper.

Streaming LLM responses with sentence-level real-time playback.

Smart Markdown cleaning so the voice does not read symbols, bold markers, or code fences.

Instant interruption â€” talk anytime and JARVIS stops speaking immediately.

Selectable LLM brain at startup.

Noise-sensitive listening using RMS thresholds + silence detection.

# System Architecture

The assistant works as three cooperating subsystems:

**JarvisEars (Speech-to-Text)**

Handles microphone input, voice activity detection, and Whisper transcription.
It captures audio in small chunks, detects when speech starts, and stops when silence crosses a set threshold.

**JarvisMouth (Text-to-Speech)**

Uses Piper TTS with chunk-based audio playback.
Can be interrupted instantly.
Also cleans Markdown, removes formatting symbols, and summarizes code blocks before speaking.

**Brain (Local LLM via Ollama)**

Ollama streams model outputs token-by-token.
As soon as a complete sentence appears (ending with ., !, or ?), it is sent to TTS for speaking.

This allows low-latency, sentence-level real-time responses.

# Why This Exists

Growing up, Tony Stark wasnâ€™t just a character â€” he was a blueprint.
Building the arc reactor in cardboard was the first iteration.

This is the adult one.

The goal wasnâ€™t simply â€œvoice commands.â€

The goal was a local AI companion that feels present, responsive, and private.

# ğŸ› ï¸ Requirements

Python 3.10+

GPU recommended (NVIDIA for CUDA acceleration)

ollama installed locally with at least one model pulled

Piper TTS model (en_US-lessac-medium.onnx)

Faster-Whisper

Python packages:

```pip install sounddevice numpy faster-whisper ollama piper-tts```

**ğŸƒ Running the Assistant**

```python3 jarvis.py```

On startup, youâ€™ll be prompted to select a model installed in Ollama.

JARVIS becomes fully interactive from that point onward.

# ğŸ§  Internals

**Speech Detection**

RMS thresholding + timed silence window lets the system know when you stop speaking.

Markdown Cleaning.

LLMs love outputting **bold**, *italics*, and code blocks.

The assistant strips these before speech so the voice output sounds natural.

**Sentence-Splitting Playback**

Incoming LLM tokens accumulate until a complete sentence is formed.

This sentence is immediately spoken, reducing perceived latency.

**Interrupt-Mechanism**

Speech is delivered in chunks (2048 samples).
Every chunk checks whether the user interrupted by talking or pressing Ctrl+C.

# ğŸ—ºï¸ Roadmap

Hotword activation (â€œJarvisâ€)

Multilingual STT + TTS

Agent modes (system control, reminders, file ops)

Memory + persistent context

# ğŸ“„ License

MIT License
